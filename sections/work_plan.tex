\chapter{Work Plan}

\section{Phases}
\subsection{Initial Overview}
\label{phase_initial_overview}

\textbf{What}: To get familiar with the LiSSA framework, I will implement another basic classifier with simple the simple prompting technique tree-of-thought (ToT) \cite{long2023LargeLanguage}. 
Currently, a zero-shot and chain-of-thought classifier are implemented. However, as \citeauthor{long2023LargeLanguage} have shown, tree-of-though style prompts can improve performance compared to chain-of-though approaches in solving logic puzzles. They have tested their approach to ToT prompting with different sized Sudoku puzzles and compared their results against zero-shot prompting as well as chain-of-thought styled one- and few-shot prompting. 


\textbf{How}: The LiSSA framework is publicly accessible in a repository \footnote{https://github.com/ArDoCo/LiSSA-RATLR/} using the MIT license. This way, existing classifiers can be used as an inspiration for development. 

The existing class \verb|classifier| is an abstraction of all classifiers and provides basic functionality to include further classifiers. Integration for different LLM providers, such as Chat-GPT, Ollama and Blablador already exists in the framework. The new classifier can be plugged in directly with little more effort than writing the request prompt and implementing the \verb|classify| function to extract the classification result out of the LLM output.

The prompts by \citewithauthor{hulbert2023UsingTreeofThought} aim to simplify tree-of-thought prompting into a singular request, instead of chaining multiple calls. I will use one of their prompts

The benchmark data to compare tree-of-thought prompting with the existing zero shot and chain-of-thought classifiers also exists in a public repository \footnote{https://github.com/ArDoCo/Benchmark} \cite{fuchss2022ArDoCoBenchmark}. 


\textbf{Why}: This work package has two major goals. Foremost, to get confident and setup with the general project structure and benchmark data. This will be a prerequisite for all future work in the LiSSA framework. Second, this will broaden the baseline to compare later results and possible improvements against.


\subsection{Naive iterative Optimization}
\label{phase_naive_iterative}
\textbf{What:} I plan to implement a naive iterative approach to prompt optimization. Many automatic prompt optimization algorithms \cite{pryzant2023AutomaticPrompt} \textit{to-do: quote more papers using iterative algorithms} depend on an iterative core loop which will be repeated until the optimized prompt performs good enough.

\textbf{How:} Another quick classifier implementation will be the simplest automatic iterative prompt optimization algorithm. The same existing abstract \verb|classifier| class can be used as in \ref{phase_initial_overview} to provide basic functionality for the classifier.

Next, a function $f$ will be required to quantify the result of the current prompt $p$ iteration. The simple approach here is to use the successful retrieval rate, by dividing the amount of correctly classified trace links by the total amount of recoverable trace links.
\[f: p \rightarrow [0, 1]\]

The benchmark data by \citewithauthor{fuchss2022ArDoCoBenchmark} also provides the gold standard for each given problem. The total amount of recoverable trace links thus is already available. \textit{to-do: how can the task set be divided into training and testing data? Use one entire tlr problem for training, the remaining for testing?}

To improve the performance, an optimization prompt is used. The naive approach is to simply ask the LLM to improve the prompt without providing further direction or information about why the prompt did not meet the requirements yet. The hopefully improved prompt will be taken into the next iteration. The act of evaluation and improving will be repeated until a threshold value is passed, meaning the prompt is considered good enough. To limit resource usage, a hard limit will also be included to put an upper bound on the optimization attempts.


\textbf{Why:} Implementing this naive iteratively optimized classifier will yield fast results to evaluate later, even if more complicated implementations might fail.  


\subsection{Automatic Prompt Optimization Based on Gradient Descent}
\label{phase_gradient_descent}
\textbf{What:} The major implementation will be an adaption of the gradient descent based automatic prompt optimization algorithm by \citewithauthor{pryzant2023AutomaticPrompt}. 

\textbf{How:} The authors have provided their python source code in a publicly accessible repository\footnote{https://github.com/microsoft/LMOps/tree/main/prompt\_optimization} under the MIT licensing. 


\textbf{Why:} 


\subsection{Evaluation and Buffer}
\label{phase_evaluation}
\textbf{What:} As this work can be seen as an expansion on the recent work of \citewithauthor{fuchss2025LiSSAGeneric} the same metrics will be used. These are the precision, recall, $F_1$-Score and $F_2$-Score. This enables an easy comparison, especially with the manual prompts designed by \citewithauthor{ewald2024RetrievalAugmentedLarge} included in the work of \citeauthor{fuchss2025LiSSAGeneric}.

\textbf{How:} Precision and recall are key measures for information retrieval tasks \cite{hayes2006AdvancingCandidate}. \textit{TODO: Explain these metricize and how they are calculated} 

Depending on how long each phase will take, it is possible to generate a bunch of different data and perform comparisons.
A simple but interesting approach is to compare different LLMs for the task. Many variations can be achieved by comparing for example how a prompt optimized by one system performs on the others. We can also compare how well each system manages to optimize the initial prompt. 
Another interesting thought is to take optimized prompts from a different system as the initial prompt. 


\textbf{Why:} Evaluating the experimental results of prompt optimization is key to making them accessible and comparable. 







\section{Artifacts}
This work will yield code to be merged into the LiSSA repository\footnote{https://github.com/ArDoCo/LiSSA-RATLR/}. The implementation of \ref{phase_initial_overview} and \ref{phase_gradient_descent} will provide a base to add further automatic prompt optimization techniques with similar building blocks.

A written report will be created to document, summarize and evaluate the results of my (automatic) prompt optimization work to retrieve trace links.


    \section{Schedule}
    \begin{ganttchart}[today=7,today label=Current Week]{1}{25}
      \gantttitle{2025}{25} \\
      %todo: unfortunately not all months have four calendar weeks :)
      %\gantttitlelist{"May","June","July","August","September","October"}{4} \\
      \gantttitlelist{18,...,42}{1} \\
      %\gantttitlelist{1,...,25}{1} \\
      \ganttgroup{Propsoal}{3}{8} \\
      \ganttbar{Litertature research}{1}{4} \\
      \ganttbar{Proposal writing}{4}{7} \\
      \ganttmilestone{Proposal presentation}{8} \\ 
      \ganttgroup{\nameref{phase_initial_overview}}{8}{12} \\
      \ganttbar{LiSSA setup}{8}{9} \\
      \ganttbar{ToT implementation}{10}{10} \\
      \ganttbar{Benchmark setup}{12}{13} \\
      \ganttgroup{Naive optimization}{13}{16} \\
      \ganttbar{Iterative classifier}{13}{13} \\

      \ganttgroup{APO gradient descent}{17}{21} \\
      \ganttbar{Implementation}{17}{19}\\
      \ganttbar{Thesis writing}{19}{23} \\
      
      \ganttgroup{\nameref{phase_evaluation}}{15}{25} \\
      \ganttbar{Evaluation}{15}{16} 
      \ganttbar{}{20}{21}\\
      \ganttbar{Buffer}{24}{25}
      \ganttbar{}{11}{11}
      \ganttbar{}{14}{14}
      \ganttbar{}{20}{20}\\

      \ganttmilestone{Thesis hand-in}{25}

      \ganttlink{elem9}{elem14}
      \ganttlink{elem11}{elem15}
      \ganttlink{elem7}{elem14}
    \end{ganttchart}