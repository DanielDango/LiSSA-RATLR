\chapter{Concept}
\label{conecpt}

In order to explore prompt engineering

\section{Initial Overview}
To get familiar with the LiSSA Framework, I will implement basic classifiers with simple popular prompting techniques.
Currently, a zero-shot and chain-of-thought classifier are implemented. In order to provide a broader baseline to compare later results against 

\section{Naive Iterative Optimization}
Next, I plan to implement a naive iterative approach to prompt optimization. Many automatic prompt optimization algorithms depend on a iterative core loop which will be repeated until the optimized prompt performs better than some metric or a maximum amount of iterations has been reached.

To improve the performance, an optimization prompt is used. The naive approach is to simply prompt the LLM to improve the prompt. The hopefully improved prompt will be taken into the next iteration.

\section{Automatic Prompt Optimization Using Gradient Descent}
Based on the work of \citewithauthor{pryzant2023automatic} I will implement a more sophisticated prompt optimization algorithm into the LiSSA framework. The authors have provided their source code in a publicly accessible repository \footnote{https://github.com/microsoft/LMOps/tree/main/prompt\_optimization} under the MIT licensing. They propose the Prompt Optimization with Textual Gradients (ProTeGi) algorithm. This entire section is based on their work.

The ProTeGi algorithm takes an initial prompt $p$ and training data $\{(x_1, y_1), \dots, (x_n, y_n)\}$ consisting of input and output. They \directQuote[sec. 2]{assume access to a black box LLM API [...] which returns a likely text continuation y of the prompt formed by concatenating p and x}{pryzant2023automatic}. They then iteratively optimize the initial prompt p to produce an approximation of the most optimized prompt for the given task. In order to optimize the prompt, a function is required, to compute deviance between the actual output $y$ and expected output $y_i$ as a numeric value.

\begin{figure}[h]
\centering
\includegraphics[width=12cm]{logos/sdqlogo}
\caption{TODO: Overview of the iterative optimization loop}
\label{fig:sdqlogo}
\end{figure}

\textit{TODO: Continue explanation of APO with gradient descent}

\section{Evaluation and Buffer}
In order to evaluate the performance of different optimized prompts, the benchmark data from \citeauthor{fuchss2022establishing} \cite{fuchss2022establishing} will be used.

As this work can be seen as an expansion on the recent work of \citewithauthor{fuchss2025lissa} the same metrics will be used. These are the precision, recall, $F_1$-Score and $F_2$-Score. This enables an easy comparison, especially with the manual prompts designed by \citewithauthor{ewald2024retrieval}.

Precision and recall are key measures for information retrieval tasks \cite{hayes2006advancing}. 
\textit{TODO: Explain these metricize and how they are calculated}

Depending on how long each phase will take, it is possible to generate a bunch of different data and perform comparisons.
A simple but interesting approach is to compare different LLMs for the task. Many variations can be achieved by comparing for example how a prompt optimized by one system performs on the others. We can also compare how well each system manages to optimize the initial prompt. 
Another interesting thought is to take optimized prompts from a different system as the initial prompt. 