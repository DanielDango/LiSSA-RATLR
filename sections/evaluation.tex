%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability
%% Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.6, 2024-06-07

\chapter{Evaluation}
\label{ch:Evaluation}

\Todo{Talk about evaluation in general}

\section{Evaluation Approach as in Proposal Document}
\label{approach:sec:evaluation}
Evaluating the experimental results of prompt optimization is central to making them accessible and comparable.
Precision~(\autoref{eq:precision}) and recall~(\autoref{eq:recall}) are key measures for information retrieval tasks~\cite{hayes2006AdvancingCandidate}.
They are commonly used by many authors to compare different approaches to \TLR.
Further the \fone and \ftwo, using a fixed $\beta$ in \autoref{eq:f_beta}, are used often.
I will use these four measures.
\citewithauthor{hey2025RequirementsTraceability} and \citewithauthor{fuchss2025LiSSAGeneric} have used these when evaluating their results on the same data sets I intend to use.
This way comparrision will be very simple.


%\textit{TODO: Explain these metricize and how they are calculated instead of direct quoting and formatting formulas}
\directQuote[sec. 7.1]{Precision measures the ratio of correctly predicted links and all predicted links. Recall measures the ratio of correctly predicted links and all relevant documents. It shows an approach’s ability to provide correct trace links. The \fone is the harmonic mean of precision and recall. The Metrics are calculated in the following way, where TP is true positives, FP is false positives, and FN is false negatives.}{ewald2024RetrievalAugmentedLarge}

\begin{align}
    \label{eq:precision}
    Precision = \frac{TP}{TP + FP} &\\
    \label{eq:recall}
    Recall = \frac{TP}{TP + FN} &\\
    \label{eq:f_beta}
    f_{\beta} = (1 + \beta^2) \cdot \frac{Precision \cdot Recall}{(\beta^2 \cdot Precision) + Recall}
\end{align}



Depending on how long each implementation phase will take, it is possible to generate a bunch of different data and performance comparisons.
A simple yet interesting approach is to compare different LLMs for the task.
Many variations can be achieved by comparing, for example, how a prompt optimized by one system performs on the others.
We can also compare how well each system manages to optimize the initial prompt.
Another interesting thought is to take optimized prompts from a different system as the initial prompt.

Varying testing data is also plausible.
Including artifacts and their links from different projects into a singular data set might have the potential to yield more general but less optimized prompts.
The exact scope of the evaluation remains to be seen.
It will be dynamically adjusted when actual implementations exist and are ready to test.


\section{Setup}
\label{sec:Evaluation:setup}
To evaluate the \APE algorithms proposed in \Todo{add link to approach}, a multitude of different datasets will be used.
They are taken from \citewithauthor{hey2025ReplicationPackage}'s replication package.
Some of the gold standard files were modified to provide consistency between artifact naming and the gold standard reference.
The actual contents are not affected by this.
The \ac{CCHIT} dataset was omitted, as it differs from the others in that it does not link high-level artifacts with low-level artifacts.
An overview of the sets used can be seen in \autoref{tab:dataset_overview}.
\directQuote{Datasets comprise \omitt high-level requirements (HLR), low-level requirements (LLR) \omitt. Percentage of linked source and target artifacts in the gold standard is given in brackets.}{hey2025RequirementsTraceability}
\Todo{add citation to dataset acronyms; add full names and mention them here as well?}
\begin{table}[]
    \centering
    \begin{tabular}{lccccc}
        & \multicolumn{2}{c}{Artifact Type} & \multicolumn{3}{c}{Number of Artifacts} \\
        \cmidrule(lr){ 2-3 } \cmidrule(lr){ 4-6 }
        Dataset   & Source & Target & Source     & Target     & Traceability Links \\
        \arrayrulecolor{kit-gray30} \midrule \arrayrulecolor{black}
        \acl{CM-1NASA}  & HLR    & LLR    & 22 (86\%)  & 53 (57\%)  & 45   \\
        \acl{Dronology} & HLR    & LLR    & 99 (93\%)  & 211 (99\%) & 220  \\
        \acl{GANNT}     & HLR    & LLR    & 17 (100\%) & 69 (99\%)  & 68   \\
        \acl{MODIS}     & HLR    & LLR    & 19 (63\%)  & 49 (63\%)  & 41   \\
        \acl{WARC}      & HLR    & LLR    & 63 (95\%)  & 89 (89\%)  & 136  \\
        \arrayrulecolor{kit-gray30} \midrule \arrayrulecolor{black}
    \end{tabular}
    \caption{Overview of the datasets adjusted from \citeauthor{hey2025RequirementsTraceability}~\cite[Table 1]{hey2025RequirementsTraceability}}
    \label{tab:dataset_overview}
\end{table}


Several \LLMs will be used.
The focus will be on \OAI's \gpt and \gptmini models.
Compared to the locally hosted \codellama and \llama models by Meta AI, they enable faster evaluation with parallel requests instead of limitations through the host-hardware.
\API access is already implemented in the \LiSSAf.
In addition to the four models used by \citewithauthor{hey2025RequirementsTraceability} in their preceding work, current models will also be considered for evaluation.
\OAI recently introduced their \gptf model.

The \LiSSAf enables the usage of different embedding models for evaluation.
However, the singular embedding model which will be used is \ac{text-embedding} by \OAI.
As of this publication, it is the most up-to-date text embedding model by \OAI and was also used by \citeauthor{hey2025RequirementsTraceability}, thus improving comparability.

The similarity retriever in the \LiSSA pipeline will also not be modified for this evaluation.
The default cosine-similarity-retriever will be utilized.
\Todo{What is a cosine similarity retriever?}

Last but not least, the different \APE algorithms presented in \autoref{ch:Approach} will be added as an additional degree of freedom compared to the baseline evaluation.
Depending on the implementation, different variables, such as mainly the optimization prompt, will be introduced.
Unless specified, each evaluation will assume that the model used to optimize the prompt will be used to classify the \TLs later to derive the quantitative metrics for evaluation.


\section{Naive Prompt Optimization}
\label{sec:Evaluation:naive_optimization}

The initial thought people may have when thinking about prompt optimization is to ask the \LLM to optimize the prompt before usage.
To utilize this very simple approach, just two prerequisites are required.
Firstly, we need our prompt, which is to be optimized.
\autoref{prompt:yes_no} was chosen here as the initial prompt.
It is a \KISS binary classification prompt.
As the \LiSSAf has already employed this prompt for their simple classifier, I decided to start the optimization process with this prompt as well.
Secondly, an optimization prompt is needed.
Therefore, \autoref{prompt:initial_optimization} has been arbitrarily selected.
Unlike \autoref{prompt:yes_no}, this prompt has not been used in research yet to my knowledge.
The optimization prompt was designed by me, while implementing the naive prompt optimization approach.
Sentence completion through GitHub Copilot based on the \ac{gpt-copilot} model was also utilized.

\begin{prompt}{Simple Optimization Prompt}{initial_optimization}
    \\
    \input{prompts/optimization-prompt-simple}
\end{prompt}

With these two prompts, the naive prompt optimization can be evaluated.
The results seemed to be quite consistent across different tested \LLMs.
\Todo{Trenne Eval Setup und Eval Results.}
I will present the optimized prompts by \OAI's \gpt model as an example of these.
\Todo{Add other models' outputs to the appendix?}

\subsection{Optimized Prompt Analysis}
\label{subsec:Evaluation:naive_optimization:optimized-prompt-analysis}
\Todo{ggf zuerst die Tabelle und dann die Analyse der Prompts.}
The first application of the optimization prompt usually yielded an inclusion of the source and target types into the text of the prompt.
\Todo{was meinst du mit "usually"? include prove for this statement in the appendix?}
This was possible as a set of training data was also provided for the following, more sophisticated, prompt optimization approaches.
Further, the very simple classification question \Quote[\autoref{prompt:yes_no}]{Are they related?} was also expanded to be more specific for the \TLR problem.
The result can be seen in \autoref{prompt:yes_no_simple}.
The optimized prompt only depends on the input prompt and the domain of the source and target elements.
This very prompt is thus used for all simple \gpt evaluations in \autoref{tab:naive_optimization} for the domain of \RtR.
\Todo{Avoid forward references.}

\begin{prompt}{\KISS Single Naive Optimization Step}{yes_no_simple}
    \\
    \input{prompts/SimpleAndRepeatedOptimization/results-prompt-optimizationWARC_simple_gpt_gpt-4o-2024-08-06_0.json_148ac023-eccc-3665-95ce-1a6f2dfb92ff}
\end{prompt}

As seen in \autoref{tab:naive_optimization}, application of the optimization prompt in the column \Quote{simple} seems to generally improve the \fone.
By repeatedly applying the optimization prompt to the optimized prompt of the previous iteration, we can push this optimization further.
This yields \autoref{prompt:yes_no_iterative}
This particular prompt was again generated by \gpt using five iteration steps.
Once more, the behavior of different \LLMs is quite similar here.
We can see that mostly longer and more detailed instructions are included on how to find \TLs.

\begin{prompt}{\KISS Iterative Naive Optimization \gpt}{yes_no_iterative}
    \\
    \input{prompts/SimpleAndRepeatedOptimization/results-prompt-optimizationWARC_iterative_gpt_gpt-4o-2024-08-06_0.json_55e9705f-3980-3580-8272-a8333ddeae93}
\end{prompt}

However, unfortunately, this prompt does not necessarily perform better for our task.
The performance of this prompt, as seen in the \Quote{iterative} column, seems to be generally worse than just the singular optimization application.
What is especially noteworthy is that for \llama, the classification fails with this prompt.

\subsection{Systematic Evaluation Results}
\label{subsec:Evaluation:naive_optimization:systematic-evaluation-results}

The systematic evaluation results for varying models and datasets are presented in \autoref{tab:naive_optimization}.
The \Average and \WeightedAverage are also included.
The \Average value is computed across all datasets, while the \WeightedAverage factors the dataset size in as well.
The amount of \TLs in the gold standard reference for the dataset, seen in \autoref{tab:dataset_overview}, is used as the metric for dataset size.

\autoref{tab:naive_optimization} \Todo{Consider removing the 1 - 0 - 0 column. bietet hier wenig mehrwert; ggf nur in text packen und dann aus der tabelle raus. sorgt auch dafür, dass die andere dinge außer 1.0 fett sind.}

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.4}
    \input{tables/SimpleAndRepeatedOptimization}
    \renewcommand{\arraystretch}{1}
    \caption{Naive prompt optimization approach prompting the model to optimize the classification prompt.}
    \label{tab:naive_optimization}
\end{table}

\newpage


\section{Simple Feedback Optimization}
\label{sec:Evaluation:simple_feedback_optimization}

The \APE process with feedback, as described in \autoref{ch:Approach} \Todo{Fix ref}, utilizes more degrees of freedom than the naive approach in \autoref{sec:Evaluation:naive_optimization}.
To describe the different configurations used in this evaluation section, the parameters in \autoref{eq:feedback_parameters} will be used.
\Todo{Are default values mentioned here or in the corresponding approach text?}

\autoref{eq:feedback_parameters}\Todo{Consider removing the textual description of the parameters and just keep the equations.}

\begin{figure}
    \raggedright
    \caption{Configurable parameters used in the \APE process.}
    \begin{equation}
        \label{eq:feedback_parameters}
        \begin{aligned}
            \iter := & \ \text{\iterl} \\
            & \ \text{the prompt will be optimized \iter times} \\
            \feed :=    & \ \text{\feedl} \\
            & \ \text{\feed examples of misclassified \TLs will be provided}
        \end{aligned}
    \end{equation}
\end{figure}

In addition to the optimization prompt used in \autoref{sec:Evaluation:naive_optimization}, an extended optimization prompt will be used here.
\autoref{prompt:feedback_initial} shows the additional details included in this prompt, which is added to the optimization prompt \autoref{prompt:initial_optimization} of \autoref{sec:Evaluation:naive_optimization}.
This will provide the \LLM with more context on where the prompt failed to classify correctly.
The misclassified \TLs will be provided as additional context.
The \LLM can then use this information to improve the prompt.
Up to \feedl misclassified \TLs will be provided.
The optimization process will only utilize a subset of the dataset, as explained in \autoref{ch:Approach} \Todo{fix ref}.
So fewer than \feedl misclassified \TLs may be available.
The parameter \iterl will be used to limit the number of optimization iterations.
In this evaluation section, \iterl was always fully utilized, as the required \fone threshold of $1.0$ for the training subset \Todo{was genau ist hier jetzt eigentlich das training subset (wie groß ist das außerdem)} was never reached.

\begin{prompt}{Feedback Prompt}{feedback_initial}
    \\
    \input{prompts/optimization-prompt-feedback}
\end{prompt}

\subsection{Optimized Prompt Analysis}
\label{subsec:Evaluation:simple_feedback:optimized-prompt-analysis}

As the model now possesses additional knowledge of the dataset through the misclassified \TLs, we can see that they start to diverge more, compared to the naive optimization approach in \autoref{sec:Evaluation:naive_optimization}.
To ease comparison, the results of \gpt are presented here again.
\autoref{prompt:feedback_optimized_3_1} shows the result of one optimization run with $\feed=3$ and $\iterl=1$.
Detailed instructions on how to classify \TLs are generated by the \LLM.
This is quite common across different models and parameterization for the early iterations.
\Todo{source?}
The prompt is also adapted to the \RtR domain as indirectly instructed through the optimization prompt.

\begin{prompt}{Optimized Prompt \feedl = 3, \iterl = 1}{feedback_optimized_3_1}
    \\
    \input{prompts/FeedbackOptimizer/results-prompt-optimizationWARC_feedback_gpt_gpt-4o-2024-08-06_0_mi1_fs3.json_0792a6e5-6b80-373b-b4e7-4214070fb896}
\end{prompt}

However, later iteration steps start to include more overfitted instructions to the provided misclassified \TLs.
During the evaluation, I observed that the optimized prompts of the previous step were often still unable to classify the same \TLs correctly.
\Todo{Can I add evidence for this?}
This is likely contributing to the overfitting of the prompt to the provided misclassified \TLs.
\autoref{prompt:feedback_optimized_5_10} in the appendix shows the full result of one optimization run with $\feed=5$ and $\iterl=10$.
The full optimized prompt can be found in the appendix.
The \LLM provided 30 indicators for classification.
One of these is obviously fitted to the WARC dataset, explicitly mentioning it in the first indicator.
A fitting to the \RtR domain is also present with indicators pointing to requirements directly.
This can be seen in \autoref{prompt:feedback_optimized_5_10_short}

\begin{prompt}{Shortend Optimized Prompt \feedl = 5, \iterl = 10}{feedback_optimized_5_10_short}
\\
\omitt \newline
1. Look for shared terminology or concepts, such as specific file names, structures, or functionalities. Pay attention to terms like "single header file," "universal header," and specific file names like "warc.h."\newline
\omitt \newline
8. Look for implicit relationships where one requirement might imply the necessity of the other, even if not explicitly stated. \newline
\omitt
\end{prompt}

\subsection{Systematic Evaluation Results}
\label{subsec:Evaluation:simple_feedback_optimization:systematic-evaluation-results}

The systematic evaluation results for varying models, datasets, and parameters are presented in \autoref{tab:feedback_optimizer}.
Like in \autoref{subsec:Evaluation:naive_optimization:systematic-evaluation-results}, the \Average and \WeightedAverage are also included.
Noteworthy is that \llama once again fails to comply with the instruction to leave the output format as \textquotesingle yes \textquotesingle or \textquotesingle no \textquotesingle for the \Dronology dataset. \Todo{Include in Appendix?}
Also, compared to the naive optimization approach in \autoref{tab:naive_optimization}, more iteration steps actually provide a benefit.

\begin{landscape}
    \begin{table}
        \centering
        %TODO: Careful, currently the tabularx width is manually replaced with \hsize in the table to adjust to landscape mode
        \renewcommand{\arraystretch}{1}
        \input{tables/FeedbackOptimizer}
        \renewcommand{\arraystretch}{1}
        \caption{Naive prompt optimization approach considering previous misclassified \TLs}
        \label{tab:feedback_optimizer}
    \end{table}
\end{landscape}


\section{Varying the Optimization Prompt}
\label{sec:Evaluation:varying-the-optimization-prompt}
In a recent paper by \citewithauthor{zadenoori2025AutomaticPrompt} the authors discussed the application of \APE for requirement classification.
They took varying initial prompts to run the optimization process with Meta's open source \llama model.
They were able to improve \fone and \ftwo scores by 5 \% and 9 \% respectively to around 80 \%.
Their \APE process is generally aligned with the general iterative optimization loop visualized in \autoref{fig:iterative_core_loop}.

Their prompt is designed to optimize classification prompts by enhancing the explanations of categories within the prompt.
As it also utilizes feedback from misclassified \TLs, it can be used as an alternative optimization prompt for the naive feedback optimization approach.
They used the optimization prompt in \autoref{prompt:zadenoori_optimization}.
The prompt is tailored to their \CoT classification prompt, comprising a five-step pipeline for requirement classification.

\Todo{Ich würde in dem Kapitel generell den Fokus darauf legen, nicht immer alle Prompts zu listen, sondern den fokus auf , was ist da wichtig. Z.B.: Warum ist hier der Komplette prompt; was bringt das dem leser? Wenn der Leser nicht den Kompletten Prompt brauch -> dann appendix. und hier verkürzt.}

\begin{prompt}{Excerpt of \citeauthor{zadenoori2025AutomaticPrompt}'s Optimization Prompt}{zadenoori_optimization_short}
    \\
    \omitt \newline
    the steps in the optimized prompt must remain exactly the same \newline
    \omitt \newline
    ensure all content remains within the existing steps and does not extend beyond them \newline
    \omitt \newline
\end{prompt}

We can see that \citeauthor{zadenoori2025AutomaticPrompt} used some similar elements to my optimization prompt.
Just as I instructed the model, that the in and output format are not to be modified, to ensure correct classification with the used classifiers, they also included this in \autoref{prompt:zadenoori_optimization_short}
Further, the second instruction \autoref{prompt:zadenoori_optimization_short} is intended to negate behavior such as in \autoref{prompt:feedback_optimized_5_10}.
However, they also gave the \LLM more specific instructions in how to optimize the prompt.
Their classification tasks include more different outputs than just the simple yes and no of \autoref{prompt:yes_no}.
The \LLM is instructed to add details to these classes.
\autoref{prompt:warc_zandoori_optimized} illustrates what this can look like for our simple \TLR \RtR task.
This full prompt is kept in the appendix.
Here we will focus on the most important details shown in \autoref{prompt:warc_zandoori_optimized_short}

As their optimization prompt is more detailed than the one used in \autoref{sec:Evaluation:simple_feedback_optimization}, it is used as an alternative optimization prompt for the naive optimization approach.
The initial prompt \autoref{prompt:yes_no} in my work does not incorporate this multistep process.
It needs to be adjusted to be used for \RtR prompt optimization.
The modified version of this prompt can be found in the appendix under \autoref{prompt:zadenoori_optimization_modified}
As seen in \autoref{tab:zadenoori_optimization}, the results of this optimization approach are quite similar to the ones of \autoref{sec:Evaluation:naive_optimization}.

\subsection{Optimized Prompt Analysis}
\label{subsec:Evaluation:varying-the-optimization-prompt:optimized-prompt-analysis}

An excerpt of the optimized prompt generated with the \gpt model for the \WARC dataset can be seen in \autoref{prompt:warc_zandoori_optimized_short}.
We can see that the \LLM focused on adding explanations to the possible classification results.
While this does include general instructions, overfitting is becoming quite obvious as well.
Some sections from the prompt are specified for the \WARC dataset, but might also find appliance elsewhere too.
As such, \fone scores during the \APE process on the reduced training set started to diverge more from the actual performance in \autoref{tab:zadenoori_optimization} than the previous prompt in \nameref{sec:Evaluation:simple_feedback_optimization}.

\begin{prompt}{Opimized WARC prompt \feedl = 3, \iterl = 5}{warc_zandoori_optimized_short}
    \\
\omitt
Class Explanations:\newline
\newline
1. \*\*Yes\*\*:
\omitt \newline
if both artifacts mention a single header file required for a software tool or application based on libwarc, they are related. \newline
\omitt \newline
they all revolve around the concept of a single header file, "warc.h", which serves as a universal entry point \newline
\omitt
\newline
2. \*\*No\*\*: \omitt
\newline
Use these examples to guide your classification decisions, ensuring that implicit connections are recognized and appropriately classified as \textquotesingle{}yes\textquotesingle{}.
\end{prompt}

\subsection{Systematic Evaluation Results}
\label{subsec:Evaluation:varying-the-optimization-prompt:systematic-evaluation-results}

The systematic evaluation results for varying models, datasets, and parameters are presented in \autoref{tab:zadenoori_optimization}.
Once again, the \Average and \WeightedAverage are also included.
The results are quite similar to the naive feedback optimization approach in \autoref{tab:feedback_optimizer}.
The \fone scores are slightly higher on average.
A prevalent issue remains that more iteration steps often do not yield better results.

\begin{landscape}
    % Todo: Careful, currently the tabularx width is manually replaced with \hsize in the table to adjust to landscape mode
    \begin{table}
        \centering
        \renewcommand{\arraystretch}{1}
        \input{tables/ZadenooriOptimizationPrompt}
        \renewcommand{\arraystretch}{1}
        \caption{Naive prompt optimization approach using the optimization prompt by \citewithauthor{zadenoori2025AutomaticPrompt}}
        \label{tab:zadenoori_optimization}
    \end{table}
\end{landscape}

\section{Gradient Descent Based Automatic Prompt Optimization }

The last prompt optimization algorithm implemented during this thesis is based on work by \citewithauthor{pryzant2023AutomaticPrompt}.
They have implemented this algorithm in Python.
I adapted it to the \LiSSAf for use in the optimization module.
As this algorithm is the most sophisticated one, it is expected to yield the best results.
The implementation has many configurable parameters.
In this evaluation, not every parameter will be altered.
Instead, the default values as by \citeauthor{pryzant2023AutomaticPrompt}'s implementation will widely be used.


Like with previous evaluations, we will alter the number of iteration steps performed in the algorithm.
We can see for several entries of \autoref{tab:gradient}, that the precision, recall and \fone remains identical between two adjacent iteration steps.
This indicates in most cases, that the identical optimized prompt was found.
Since \LLM requests are cached in the \LiSSAf this suggests, that the \APE algorithm was not able to improve the prompt in this iteration step.
\autoref{tab:gradient} does not visualize other information that is passed on between iteration steps though.
Thus the \APE algorithm was still able to alter the optimized output prompt further in later steps, instead of stalling with this candidate.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \input{tables/GradientOptimizationPryzant}
    \renewcommand{\arraystretch}{1}
    \caption{Gradient Descent based Automatic Prompt Optimization}
    \label{tab:gradient}
\end{table}

\begin{prompt}{Opimized WARC prompt \iterl = 3}{warc_gradient_optimized}
    \\
    \input{prompts/GradientOptimizationPryzant/results-prompt-optimizationWARC_gradient_gpt-4o-mini-2024-07-18_mi3.json_f3901225-31d7-36d5-86ee-d400c573de1a}
\end{prompt}

To examine one of the optimized prompts closer we can take a look at \autoref{prompt:warc_gradient_optimized}.
It is also the prompt with the single highest \fone in \autoref{tab:gradient}.
We can note, that compared to optimized prompts of the previous \nameref{sec:Evaluation:simple_feedback_optimization} with \autoref{prompt:feedback_optimized_5_10} for example, the optimized prompt is significantly shorter.
Furthermore, the prompt does not show signs of overfitting for features unique to the \WARC dataset.
The input and output formats of the prompt are left intact, as is required for prompts to perform well within the constraints of the \LiSSAf.
The major changes over \autoref{prompt:yes_no} are simply the explanatory text describing when artifacts should have a \TL existing between them.
Indications of this prompt being fitted to \RtR datasets are present.
The optimized prompt lists three criteria for linked artifacts.
\begin{enumerate}
    \item purpose
    \item functionality
    \item requirements
\end{enumerate}

While the purpose is very general applicable, the functionality already requires domains illustrating features or capabilities.
They might not be present in high level artifacts specifying the project outline, such as architectural diagrams.
Last but not least requirements require specific knowledge of the project.
Evaluating \TLs between documentation and code would likely hide many requirements or only implicitly mention them.
\Todo{This statement sounds a little far fetched. Source required? (@advisor)}

\section{Application of an Optimized Prompt to Unfamiliar Datasets}
\Todo{Add Reasoning Prompt Baseline?}
\APE with the proposed algorithms in my work is always connected to additional costs in execution time and possibly budget for \LLM tokens.
Thus the optimized prompts should ideally be generally applicable, eliminating the need for repeated optimization.
To test this, the single best-performing prompt in terms of \fone will be used to classify \TLs in the other datasets.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \input{tables/CrossPerformance}
    \renewcommand{\arraystretch}{1}
    \caption{Classification performance of an \ProTeGi optimized prompt for \WARC by \gptmini across other datasets}
    \label{tab:crossevaluation}
\end{table}

\autoref{tab:crossevaluation} illustrates the classification results with the previous most promising \autoref{prompt:warc_gradient_optimized}.
In addition to the \gptmini model, used to optimize the prompt previously, it is also tested for \gpt and \gptf.
We can see that the \fone score is improved across all datasets and models.
This suggests that \autoref{prompt:warc_gradient_optimized} performs better in this regard than the default KISS \autoref{prompt:yes_no} of \LiSSA.
Furthermore, the precision is generally improved as well.
On the other hand, recall rates are usually lower than for the unoptimized counterpart.