%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability
%% Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.6, 2024-06-07

\chapter{Evaluation}
\label{ch:Evaluation}

\Todo{Talk about evaluation in general}

\section{Evaluation Approach as in Proposal Document}
\label{approach:sec:evaluation}
Evaluating the experimental results of prompt optimization is central to making them accessible and comparable.
Precision~(\autoref{eq:precision}) and recall~(\autoref{eq:recall}) are key measures for information retrieval tasks~\cite{hayes2006AdvancingCandidate}.
They are commonly used by many authors to compare different approaches to \TLR.
Further the \fone and \ftwo, using a fixed $\beta$ in \autoref{eq:f_beta}, are used often.
I will use these four measures.
\citewithauthor{hey2025RequirementsTraceability} and \citewithauthor{fuchss2025LiSSAGeneric} have used these when evaluating their results on the same data sets I intend to use.
This way comparrision will be very simple.


%\textit{TODO: Explain these metricize and how they are calculated instead of direct quoting and formatting formulas}
\directQuote[sec. 7.1]{Precision measures the ratio of correctly predicted links and all predicted links. Recall measures the ratio of correctly predicted links and all relevant documents. It shows an approachâ€™s ability to provide correct trace links. The \fone is the harmonic mean of precision and recall. The Metrics are calculated in the following way, where TP is true positives, FP is false positives, and FN is false negatives.}{ewald2024RetrievalAugmentedLarge}

\begin{align}
    \label{eq:precision}
    Precision = \frac{TP}{TP + FP} &\\
    \label{eq:recall}
    Recall = \frac{TP}{TP + FN} &\\
    \label{eq:f_beta}
    f_{\beta} = (1 + \beta^2) \cdot \frac{Precision \cdot Recall}{(\beta^2 \cdot Precision) + Recall}
\end{align}



Depending on how long each implementation phase will take, it is possible to generate a bunch of different data and performance comparisons.
A simple yet interesting approach is to compare different LLMs for the task.
Many variations can be achieved by comparing, for example, how a prompt optimized by one system performs on the others.
We can also compare how well each system manages to optimize the initial prompt.
Another interesting thought is to take optimized prompts from a different system as the initial prompt.

Varying testing data is also plausible.
Including artifacts and their links from different projects into a singular data set might have the potential to yield more general but less optimized prompts.
The exact scope of the evaluation remains to be seen.
It will be dynamically adjusted when actual implementations exist and are ready to test.


\subsection{Datasets}
\label{approach:sec:datasets}
The datasets used for evaluation will be the same as in \citewithauthor{hey2025RequirementsTraceability}.


\section{Setup}
\label{sec:Evaluation:setup}
To evaluate the \APE algorithms proposed in \Todo{add link to approach}, a multitude of different datasets will be used.
They are taken from \citewithauthor{hey2025ReplicationPackage}'s replication package.
Some of the gold standard files were modified to provide consistency between artifact naming and the gold standard reference.
The actual contents are not affected by this.
The CCHIT dataset was omitted, as it differs from the others in that it does not link high-level artifacts with low-level artifacts.
An overview of the sets used can be seen in \autoref{tab:dataset_overview}.
\directQuote{Datasets comprise either high-level requirements (HLR), low-level requirements (LLR), requirements (R) or regulatory codes (RC). Percentage of linked source and target artifacts in the gold standard is given in brackets.}{hey2025RequirementsTraceability}

\begin{table}[]
    \centering
    \begin{tabular}{lccccc}
        & \multicolumn{2}{c}{Artifact Type} & \multicolumn{3}{c}{Number of Artifacts} \\
        \cmidrule(lr){ 2-3 } \cmidrule(lr){ 4-6 }
        Dataset   & Source & Target & Source     & Target     & Traceability Links \\
        \arrayrulecolor{kit-gray30} \midrule \arrayrulecolor{black}
        CM1-NASA  & HLR    & LLR    & 22 (86\%)  & 53 (57\%)  & 45   \\
        Dronology & HLR    & LLR    & 99 (93\%)  & 211 (99\%) & 220  \\
        GANNT     & HLR    & LLR    & 17 (100\%) & 69 (99\%)  & 68   \\
        MODIS     & HLR    & LLR    & 19 (63\%)  & 49 (63\%)  & 41   \\
        WARC      & HLR    & LLR    & 63 (95\%)  & 89 (89\%)  & 136  \\
        \arrayrulecolor{kit-gray30} \midrule \arrayrulecolor{black}
    \end{tabular}
    \caption{Overview of the datasets adjusted from \citeauthor{hey2025RequirementsTraceability}~\cite[Table 1]{hey2025RequirementsTraceability}}
    \label{tab:dataset_overview}
\end{table}


Several \LLMs will be used.
The focus will be on \OAI's \gpt and \gptmini models.
Compared to the locally hosted \codellama and \llama models by Meta AI, they enable faster evaluation with parallel requests instead of limitations through the host-hardware.
\API access is already implemented in the \LiSSAf.
In addition to the four models used by \citewithauthor{hey2025RequirementsTraceability} in their preceding work, current models will also be considered for evaluation.
\OAI recently introduced their \gptf model.
\Todo{is the gpt-5 macro set correctly?}

The \LiSSAf enables the usage of different embedding models for evaluation.
However, the singular embedding model which will be used is \ac{text-embedding} by \OAI.
As of this publication, it is the most up-to-date text embedding model by \OAI and was also used by \citeauthor{hey2025RequirementsTraceability}, thus improving comparability.

The similarity retriever in the \LiSSA pipeline will also not be modified for this evaluation.
The default cosine-similarity-retriever will be utilized.
\Todo{What is a cosine similarity retriever?}

Last but not least, the different \APE algorithms presented in \autoref{ch:Approach} will be added as an additional degree of freedom compared to the baseline evaluation.
Depending on the implementation, different variables, such as mainly the optimization prompt, will be introduced.
Unless specified, each evaluation will assume that the model used to optimize the prompt will be used to classify the \TLs later to derive the quantitative metrics for evaluation.


\section{Naive Prompt Optimization}
\label{sec:Evaluation:naive_optimization}

The initial thought people may have when thinking about prompt optimization is to ask the \LLM to optimize the prompt before usage.
To utilize this very simple approach, just two prerequisites are required.
Firstly, we need our prompt, which is to be optimized.
\autoref{prompt:yes_no} was chosen here as the initial prompt.
It is a \KISS binary classification prompt.
As the \LiSSAf has already employed this prompt for their simple classifier, I decided to start the optimization process with this prompt as well.
Secondly, an optimization prompt is needed.
Therefore, \autoref{prompt:initial_optimization} has been arbitrarily selected.
Unlike \autoref{prompt:yes_no}, this prompt has not been used in research yet to my knowledge.
The optimization prompt was designed by me, while implementing the naive prompt optimization approach.
Sentence completion through GitHub Copilot based on the \ac{gpt-copilot} model was also utilized.

\begin{prompt}{\KISS Original}{yes_no}
    \\
    \input{prompts/original-kiss}
\end{prompt}

\begin{prompt}{Simple Optimization Prompt}{initial_optimization}
    \\
    \input{prompts/optimization-prompt-simple}
\end{prompt}

With these two prompts, the naive prompt optimization can be evaluated.
The results seemed to be quite consistent across different tested \LLMs.
\Todo{Trenne Eval Setup und Eval Results.}
I will present the optimized prompts by \OAI's \gpt model as an example of these.
\Todo{Add other models' outputs to the appendix?}

\subsection{Optimized Prompt Analysis}
\label{subsec:Evaluation:naive_optimization:optimized-prompt-analysis}
\Todo{ggf zuerst die Tabelle und dann die Analyse der Prompts.}
The first application of the optimization prompt usually yielded an inclusion of the source and target types into the text of the prompt.
\Todo{was meinst du mit "usually"? include prove for this statement in the appendix?}
This was possible as a set of training data was also provided for the following, more sophisticated, prompt optimization approaches.
Further, the very simple classification question \Quote[\autoref{prompt:yes_no}]{Are they related?} was also expanded to be more specific for the \TLR problem.
The result can be seen in \autoref{prompt:yes_no_simple}.
The optimized prompt only depends on the input prompt and the domain of the source and target elements.
This very prompt is thus used for all simple \gpt evaluations in \autoref{tab:naive_optimization} for the domain of \RtR.
\Todo{Avoid forward references.}

\begin{prompt}{\KISS Single Naive Optimization Step}{yes_no_simple}
    \\
    \input{prompts/SimpleAndRepeatedOptimization/results-prompt-optimizationWARC_simple_gpt_gpt-4o-2024-08-06_0.json_411c69c3-cec3-3948-ad97-bb474536d21c}
\end{prompt}

As seen in \autoref{tab:naive_optimization}, application of the optimization prompt in the column \Quote{simple} seems to generally improve the \fone.
By repeatedly applying the optimization prompt to the optimized prompt of the previous iteration, we can push this optimization further.
This yields \autoref{prompt:yes_no_iterative}
This particular prompt was again generated by \gpt using five iteration steps.
Once more, the behavior of different \LLMs is quite similar here.
We can see that mostly longer and more detailed instructions are included on how to find \TLs.

\begin{prompt}{\KISS Iterative Naive Optimization \gpt}{yes_no_iterative}
    \\
    \input{prompts/SimpleAndRepeatedOptimization/results-prompt-optimizationWARC_iterative_gpt_gpt-4o-2024-08-06_0.json_e81eaea6-576b-337c-8a98-f2e65913a143}
\end{prompt}

However, unfortunately, this prompt does not necessarily perform better for our task.
The performance of this prompt, as seen in the \Quote{iterative} column, seems to be generally worse than just the singular optimization application.
What is especially noteworthy is that for \llama, the classification fails with this prompt.
We can see in \autoref{prompt:yes_no_llama} that the prompt successfully modified the output format to accommodate different types of relationships.
The current classifiers of the \LiSSA project expect the \LLM output to include the classification result clearly as \textquotesingle yes \textquotesingle or \textquotesingle no \textquotesingle.
Thus, while this more detailed classification of the \LLM might even improve performance, it can not be properly evaluated.
\Todo{Consider how this might be mitigated}

\begin{prompt}{\KISS Iterative Naive Optimization \llama}{yes_no_llama}
    \\
    \input{prompts/SimpleAndRepeatedOptimization/results-prompt-optimizationWARC_iterative_ollama_llama3.1_8b-instruct-fp16_0.json_28d45aa7-dc74-3651-942a-c21f7ab1e001}
\end{prompt}

\subsection{Systematic Evaluation Results}
\label{subsec:Evaluation:naive_optimization:systematic-evaluation-results}

The systematic evaluation results for varying models and datasets are presented in \autoref{tab:naive_optimization}.
The average and weighted average are also included.
The average value is computed across all datasets, while the weighted average factors the dataset size in as well.
The amount of \TLs in the gold standard reference for the dataset, seen in \autoref{tab:dataset_overview}, is used as the metric for dataset size.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.4}
    \input{tables/SimpleAndRepeatedOptimization}
    \renewcommand{\arraystretch}{1}
    \caption{Naive prompt optimization approach prompting the model to optimize the classification prompt.
    \Todo{Consider removing the 1 - 0 - 0 column. bietet hier wenig mehrwert; ggf nur in text packen und dann aus der tabelle raus. sorgt auch dafÃ¼r, dass die andere dinge auÃŸer 1.0 fett sind.}}
    \label{tab:naive_optimization}
\end{table}

\newpage


\section{Simple Feedback Optimization}
\label{sec:Evaluation:simple_feedback_optimization}

The \APE process with feedback, as described in \autoref{ch:Approach} \Todo{Fix ref}, utilizes more degrees of freedom than the naive approach in \autoref{sec:Evaluation:naive_optimization}.
To describe the different configurations used in this evaluation section, the parameters in \autoref{eq:feedback_parameters} will be used.
\Todo{Are default values mentioned here or in the corresponding approach text?}

\begin{figure}
    \raggedright
    \caption{Configurable parameters used in the \APE process. \\
    \Todo{Consider removing the textual description of the parameters and just keep the equations.}}
    \begin{equation}
        \label{eq:feedback_parameters}
        \begin{aligned}
            \iter := & \ \text{\acl{iter}} \\
            & \ \text{the prompt will be optimized \iter times} \\
            \feed :=    & \ \text{\acl{feed}} \\
            & \ \text{\feed examples of misclassified \TLs will be provided}
        \end{aligned}
    \end{equation}
\end{figure}

In addition to the optimization prompt used in \autoref{sec:Evaluation:naive_optimization}, an extended optimization prompt will be used here.
\autoref{prompt:feedback_initial} shows the additional details included in this prompt, which is added to the optimization prompt \autoref{prompt:initial_optimization} of \autoref{sec:Evaluation:naive_optimization}.
This will provide the \LLM with more context on where the prompt failed to classify correctly.
The misclassified \TLs will be provided as additional context.
The \LLM can then use this information to improve the prompt.
Up to \feed misclassified \TLs will be provided.
The optimization process will only utilize a subset of the dataset, as explained in \autoref{ch:Approach} \Todo{fix ref}.
So fewer than \feed misclassified \TLs may be available.
The parameter \iter will be used to limit the number of optimization iterations.
In this evaluation section, \iter was always fully utilized, as the required \fone threshold of $1.0$ for the training subset \Todo{was genau ist hier jetzt eigentlich das training subset (wie groÃŸ ist das auÃŸerdem)} was never reached.

\begin{prompt}{Feedback Prompt}{feedback_initial}
    \\
    \input{prompts/optimization-prompt-feedback}
\end{prompt}

\subsection{Optimized Prompt Analysis}
\label{subsec:Evaluation:simple_feedback:optimized-prompt-analysis}

As the model now possesses additional knowledge of the dataset through the misclassified \TLs, we can see that they start to diverge more, compared to the naive optimization approach in \autoref{sec:Evaluation:naive_optimization}.
To ease comparison, the results of \gpt are presented here again.
\autoref{prompt:feedback_optimized_3_1} shows the result of one optimization run with $\feed=3$ and $\iter=1$.
Detailed instructions on how to classify \TLs are generated by the \LLM.
This is quite common across different models and parameterization for the early iterations.
\Todo{source?}
The prompt is also adapted to the \RtR domain as indirectly instructed through the optimization prompt.

\begin{prompt}{Optimized Prompt \feed = 3, \iter = 1}{feedback_optimized_3_1}
    \\
    \input{prompts/FeedbackOptimizer/results-prompt-optimizationWARC_feedback_gpt_gpt-4o-2024-08-06_0_mi1_fs3.json_763f1e98-fde5-3555-8289-67e08e354d5a}
\end{prompt}

However, later iteration steps start to include more overfitted instructions to the provided misclassified \TLs.
During the evaluation, I observed that the optimized prompts of the previous step were often still unable to classify the same \TLs correctly.
\Todo{Can I add evidence for this?}
This is likely contributing to the overfitting of the prompt to the provided misclassified \TLs.
\autoref{prompt:feedback_optimized_5_10} shows the result of one optimization run with $\feed=5$ and $\iter=10$.
The full optimized prompt can be found in the appendix. \Todo{Add to appendix}
The \LLM provided 20 indicators for classification.
Several of these are fitted to the WARC dataset, explicitly mentioning it, for example, in point ten and eleven for instance.

\begin{prompt}{Optimized Prompt \feed = 5, \iter = 10}{feedback_optimized_5_10}
    \\
    \Todo{hier ggf einfach nur der relevante ausschnitt.
    In dein Replication Package sollten dann die Kompletten prompts (oder auch gerne in den Appendix).
    das ist generell glaub fÃ¼r den Lesefluss wichtig.
    hier ggf immer die ausschnitte die relevant sind, und das komplette in den appendix.}
    \input{prompts/FeedbackOptimizer/results-prompt-optimizationWARC_feedback_gpt_gpt-4o-2024-08-06_0_mi10_fs5.json_475529c1-57eb-36c3-a3fb-87ea9674a11c}
\end{prompt}

\subsection{Systematic Evaluation Results}
\label{subsec:Evaluation:simple_feedback_optimization:systematic-evaluation-results}

The systematic evaluation results for varying models, datasets, and parameters are presented in \autoref{tab:feedback_optimizer}.
Like in \autoref{subsec:Evaluation:naive_optimization:systematic-evaluation-results}, the average and weighted average are also included.
Noteworthy is that \llama once again fails to comply with the instruction to leave the output format as \textquotesingle yes \textquotesingle or \textquotesingle no \textquotesingle for the dronology dataset. \Todo{Include in Appendix?}
Also, compared to the naive optimization approach in \autoref{tab:naive_optimization}, more iteration steps actually provide a benefit.

\begin{landscape}
    \begin{table}
        \centering
        %TODO: Careful, currently the tabularx width is manually replaced with \hsize in the table to adjust to landscape mode
        \renewcommand{\arraystretch}{1}
        \input{tables/FeedbackOptimizer}
        \renewcommand{\arraystretch}{1}
        \caption{Naive prompt optimization approach considering previous misclassified \TLs}
        \label{tab:feedback_optimizer}
    \end{table}
\end{landscape}


\section{Varying the Optimization Prompt}
\label{sec:Evaluation:varying-the-optimization-prompt}
In a recent paper by \citewithauthor{zadenoori2025AutomaticPrompt} the authors discussed the application of \APE for requirement classification.
They took varying initial prompts to run the optimization process with Meta's open source \llama model.
They were able to improve \fone and \ftwo scores by 5 \% and 9 \% respectively to around 80 \%.
Their \APE process is generally aligned with the general iterative optimization loop visualized in \autoref{fig:iterative_core_loop}.

Their prompt is designed to optimize classification prompts by enhancing the explanations of categories within the prompt.
As it also utilizes feedback from misclassified \TLs, it can be used as an alternative optimization prompt for the naive feedback optimization approach.
They used the following optimization prompt in \autoref{prompt:zadenoori_optimization}.
The prompt is tailored to their \CoT classification prompt, comprising a five-step pipeline for requirement classification.

\Todo{Ich wÃ¼rde in dem Kapitel generell den Fokus darauf legen, nicht immer alle Prompts zu listen, sondern den fokus auf , was ist da wichtig. Z.B.: Warum ist hier der Komplette prompt; was bringt das dem leser? Wenn der Leser nicht den Kompletten Prompt brauch -> dann appendix. und hier verkÃ¼rzt.}

\begin{prompt}{Zadenoori Optimization Prompt}{zadenoori_optimization}
    \\
    \input{prompts/ZadenooriOptimizationPrompt}
\end{prompt}

We can see that \citeauthor{zadenoori2025AutomaticPrompt} used some similar elements to my optimization prompt.
Just as I instructed the model, that the in and output format are not to be modified, in order to ensure correct classification with the used classifiers, they also included that \Quote[\autoref{prompt:zadenoori_optimization}]{he steps in the optimized prompt must remain exactly the same}.
Further, the instruction \Quote[\autoref{prompt:zadenoori_optimization}]{ensure all content remains within the existing steps and does not extend beyond them} is intended to negate behavior such as in \autoref{prompt:feedback_optimized_5_10}.
However, they also gave the \LLM more specific instructions in how to optimize the prompt.
Their classification tasks include more different outputs than just the simple yes and no of \autoref{prompt:yes_no}.
The \LLM is instructed to add details to these classes.
\autoref{prompt:warc_zandoori_optimized} illustrates what this can look like for our simple \TLR \RtR task.

As their optimization prompt is more detailed than the one used in \autoref{sec:Evaluation:simple_feedback_optimization}, it is used as an alternative optimization prompt for the naive optimization approach.
The initial prompt \autoref{prompt:yes_no} in my work does not incorporate this multistep process.
It needs to be adjusted to be used for \RtR prompt optimization.
\Todo{The modified version of this prompt can be found in the appendix}
As seen in \autoref{tab:zadenoori_optimization}, the results of this optimization approach are quite similar to the ones of \autoref{sec:Evaluation:naive_optimization}.

\subsection{Optimized Prompt Analysis}
\label{subsec:Evaluation:varying-the-optimization-prompt:optimized-prompt-analysis}

An optimized prompt generated with the \gpt model for the WARC dataset can be seen in \autoref{prompt:warc_zandoori_optimized}.
We can see that the \LLM focused on adding explanations to each of the two possible classification results.
While this does include general instructions, overfitting is becoming quite obvious as well.
Some sections from the prompt are specified for the WARC dataset, but might also find appliance elsewhere too.
For instance, a \Quote[\autoref{prompt:warc_zandoori_optimized}]{universal interface to create WARC records and another requirement details the types of WARC records that can be created through such an interface} can be generalized to other applications.
On the other hand, solutions are also directly embedded in the prompt.
The phrase \Quote[\autoref{prompt:warc_zandoori_optimized}]{"FR 3" specifies providing functions \idots, and "SRS 7" specifies \idots, they are directly related} refers to two requirements which are present in the training set.
As such, \fone scores during the \APE process on the reduced training set started to diverge more from the actual performance in \autoref{tab:zadenoori_optimization} than the previous prompt in \nameref{sec:Evaluation:simple_feedback_optimization}.

\begin{prompt}{Opimized WARC prompt \feed = 3, \iter = 5}{warc_zandoori_optimized}
    \\
    \input{prompts/ZadenooriOptimizationPrompt/results-prompt-optimizationWARC_feedback_gpt_gpt-4o-2024-08-06_0_mi5_fs3.json_f757620e-36c1-36a1-9314-c0acb1d5432f }
\end{prompt}

\subsection{Systematic Evaluation Results}
\label{subsec:Evaluation:varying-the-optimization-prompt:systematic-evaluation-results}

The systematic evaluation results for varying models, datasets, and parameters are presented in \autoref{tab:zadenoori_optimization}.
Once again, the average and weighted average are also included.
The results are quite similar to the naive feedback optimization approach in \autoref{tab:feedback_optimizer}.
The \fone scores are slightly higher on average.
A prevalent issue remains that more iteration steps often do not yield better results.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.4}
    \input{tables/ZadenooriOptimizationPrompt}
    \renewcommand{\arraystretch}{1}
    \caption{Naive prompt optimization approach using the optimization prompt by \citewithauthor{zadenoori2025AutomaticPrompt}}
    \label{tab:zadenoori_optimization}
\end{table}