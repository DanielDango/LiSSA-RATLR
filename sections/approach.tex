\chapter{Approach}
\label{ch:Approach}
In this chapter, I will outline how my thesis will be realized and provide source code that can be merged into the \LiSSA project.
My work aims to implement three different classifiers into the framework.
First, I plan to implement a simple \ToT classifier in \autoref{approach:sec:tot} to broaden the baseline to compare later optimized prompts against.
Next, I expect to work on a reduced iterative optimizer in \autoref{approach:sec:naive_iterative} using a naive approach to optimization.
After this already provides basic components and data to evaluate the more sophisticated algorithm by \citewithauthor{pryzant2023AutomaticPrompt} will be implemented in \autoref{approach:sec:gradient_descent}.
Last but not least, I will explain how I plan to evaluate and compare my findings in \autoref{approach:sec:evaluation}.

\section{Simple Tree-of-Thought Classifier}
\label{approach:sec:tot}

\begin{figure}
    \centering
    \includesvg[width=\linewidth]{graphics/tree_of_thought}
    \caption{\Ac{ToT} visualization inspired by \citewithauthor{yao2023TreeThoughts}}
    \label{fig:ToT_visualization}
\end{figure}

\Ac{ToT} is a prompting technique exploring more different branches of thoughts than typical \CoT prompts.
As illustrated in \autoref{fig:ToT_visualization}, many branches are explored concurrently instead of following a single strain of thoughts like \CoT prompting.
The prompts by \citewithauthor{hulbert2023UsingTreeofThought} aim to simplify \ToT prompting into a singular request, instead of chaining multiple calls.
I will use one of their prompts.
The benchmark data to compare \ToT prompting with the existing zero-shot and \CoT classifiers also exists in public repositories~\cite{fuchss2022ArDoCoBenchmark, hey2025ReplicationPackage}.

The \LiSSAf is publicly accessible in a repository using the MIT license. Various classifiers are already included there.
They can be used as an inspiration for development.
The class \verb|classifier| is an abstraction of all classifiers.
It provides basic functionality to include further classifiers.
Integration for different \LLM providers, such as Chat-GPT, Ollama, and Blablador, already exists in the framework.
The new classifier can be plugged in directly with little more effort than writing the request prompt and implementing the \verb|classify| function to extract the classification result out of the \LLM output.

This work package has two major goals.
Foremost, to get confident and set up with the general project structure and benchmark data.
This will be a prerequisite for all future work in the LiSSA framework.
Second, this will broaden the baseline to compare later results and possible improvements against.




\section{Naive Iterative Optimizing Classifier}
\label{approach:sec:naive_iterative}
Another classifier implementation will be the simplest automatic iterative prompt optimization algorithm.
The same existing abstract \verb|classifier| class can be used as in \autoref{phase_initial_overview} to provide basic functionality for the iteratively optimized classifier.

Next, a function $f$ will be required to quantify the result, also referred to as performance, of the current prompt $p$ iteration, called to a large language model $llm$ using a set of training data $X$.
The simple approach here is to use the successful retrieval rate by dividing the number of correctly classified trace links by the total number of recoverable trace links.
The function $f$ will need to have the same range for any set of training data to ensure the threshold value $t$ will always be viable.

\begin{align}
        f: llm(p, X) \rightarrow [0, 1] \\
        t \in [0, 1]
\end{align}

The benchmark data by \citewithauthor{hey2025ReplicationPackage} also provides the gold standard for each given problem.
The total amount of recoverable trace links is thus already available. 
%\textit{to-do: how can the task set be divided into training and testing data? Use one entire TLR problem for training, the remaining for testing?}

To improve the performance, an optimization prompt is used.
As visualized in \autoref{fig:iterative_core_loop} the expected outputs for training data may also be used to improve the prompt.
The most naive approach is to simply ask the LLM to improve the prompt without providing further direction or information about why the prompt did not meet the requirements.
The hopefully improved prompt will be taken into the next iteration.
The act of evaluation and improving will be repeated until a threshold value is passed, meaning the prompt is considered good enough.
To limit resource usage, a hard limit will also be included to put an upper bound on the optimization attempts.

\begin{figure}
    \centering
    \includesvg[width=0.7\linewidth]{graphics/Iterative_core_loop}
    \caption{Visualization of a simple iterative optimization algorithm}
    \label{fig:iterative_core_loop}
\end{figure}

Implementing this naive, iteratively optimized classifier will yield fast results to evaluate later, even if more complicated implementations might fail.
The core concepts also apply to \autoref{phase_gradient_descent} and can be reused.


\section{Automatic Prompt Optimization with Gradient Descent}
\label{approach:sec:gradient_descent}
The existing iterative optimization implementation from~\autoref{phase_naive_iterative} can be used as a foundation for the implementation, as the core concepts remain the same.
Refer to~\autoref{sec:gradient_descent} for details of the gradient-descent-based optimization algorithm.

\citewithauthor{pryzant2023AutomaticPrompt} have provided their Python source code in a publicly accessible repository\footnote{https://github.com/microsoft/LMOps/tree/main/prompt\_optimization} under the MIT license.
The project code can be of great help when adapting their algorithm to the LiSSA Java framework.
The actual implementation of gradient descent and the bandit-selection algorithm will be the main focus, in order to steer optimized prompts in the right direction.

The primary idea of this thesis is to explore automated optimization.
The naive approach may yield good results already.
Feeding more information and processing logic into the optimization system is expected to present better results.
Especially suitable results may be reached faster by doing more processing on the machine instead of expensive API calls to the large language model.


\section{Evaluation}
\label{approach:sec:evaluation}
 Evaluating the experimental results of prompt optimization is central to making them accessible and comparable. 
 Precision~(\autoref{eq:precision}) and recall~(\autoref{eq:recall}) are key measures for information retrieval tasks~\cite{hayes2006AdvancingCandidate}.
 They are commonly used by many authors to compare different approaches to \TLR. 
 Further the $f1-score$ and $f2-score$, using a fixed $\beta$ in \autoref{eq:f_beta}, are used often.
I will use these four measures.
\citewithauthor{hey2025RequirementsTraceability} and \citewithauthor{fuchss2025LiSSAGeneric} have used these when evaluating their results on the same data sets I intend to use.
This way comparrision will be very simple.
 

%\textit{TODO: Explain these metricize and how they are calculated instead of direct quoting and formatting formulas}
\directQuote[sec. 7.1]{Precision measures the ratio of correctly predicted links and all predicted links. Recall measures the ratio of correctly predicted links and all relevant documents. It shows an approachâ€™s ability to provide correct trace links. The F1 score is the harmonic mean of precision and recall. The Metrics are calculated in the following way, where TP is true positives, FP is false positives, and FN is false negatives.}{ewald2024RetrievalAugmentedLarge}

\begin{align}
    \label{eq:precision}
    Precision = \frac{TP}{TP + FP} &\\
    \label{eq:recall}
    Recall = \frac{TP}{TP + FN} &\\
    \label{eq:f_beta}
    f_{\beta} = (1 + \beta^2) \cdot \frac{Precision \cdot Recall}{(\beta^2 \cdot Precision) + Recall}
\end{align}



Depending on how long each implementation phase will take, it is possible to generate a bunch of different data and performance comparisons.
A simple yet interesting approach is to compare different LLMs for the task.
Many variations can be achieved by comparing, for example, how a prompt optimized by one system performs on the others.
We can also compare how well each system manages to optimize the initial prompt.
Another interesting thought is to take optimized prompts from a different system as the initial prompt.

Varying testing data is also plausible.
Including artifacts and their links from different projects into a singular data set might have the potential to yield more general but less optimized prompts. 
The exact scope of the evaluation remains to be seen.
It will be dynamically adjusted when actual implementations exist and are ready to test.


\subsection{Datasets}
\label{approach:sec:datasets}
The datasets used for evaluation will be the same as in \citewithauthor{hey2025RequirementsTraceability}.
