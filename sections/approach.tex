\chapter{Approach}
\label{ch:Approach}

This chapter outlines how the research questions of the thesis are addressed.
The produced source code can seamlessly integrate into the \LiSSA project.
In this thesis four different methods are used to test their \TLR performance.
First, a simple \ToT style prompt is used for classification instead of the existing \zeroshot or \CoT style prompts.
The usage of this prompting style is presented in \autoref{sec:Approach:tot}.
Next, naive iterative prompt optimization approaches with and without feedback are be elaborated in \autoref{approach:sec:naive_iterative}.
They provide basic functionality upon which the more sophisticated \ProTeGi algorithm in \autoref{approach:sec:gradient_descent} is based.


\section{Tree-of-Thought Prompting}
\label{sec:Approach:tot}

\Ac{ToT} is a prompting technique exploring more different branches of thoughts than typical \CoT prompts.
This can be achieved by chaining multiple requests to a \LLM~\cite{long2023LargeLanguage}.

\begin{figure}
    \centering
    \includegraphics{graphics/tree_of_thought.drawio}
    \caption{\Ac{ToT} visualization inspired by \citewithauthor{yao2023TreeThoughts}}
    \label{fig:ToT_visualization}
    Highlighted in green are different prompting techniques.
    Their related paths are enclosed in the light blue box.
    Highlighted in yellow is data, with the input which is queried using one of the prompting techniques to the \LLM and output representing the returned result of this query.
    The rounded boxes represent thoughts of the \LLM.
    Orange thoughts are considered inconsistent and will be disregarded in the next step.
    Arrows represent the processing flow of data.

\end{figure}

In \autoref{fig:ToT_visualization} we can see the three different prompting techniques highlighted in green.
Among them are aside from the \ToT prompt also a \zeroshot and \CoT prompt, to visualize the differences.
The input data, highlighted in yellow, is queried with one of the prompting techniques to the \LLM.
In our case this can be a pair of requirements, for which the output should state whether a \TL exists between them.
Examining the three different prompting techniques, we note a varying amount of thoughts before the output is reached.
Thoughts are represented by white and orange rounded boxes, with orange ones being discarded during the thought process.
As for the zero-shot prompt the \LLM reaches their conclusion without a thought process.
The \CoT style prompt results in a linear thought path.
Thus, if any thought deviates, the \LLM might be led to a false conclusion~\cite{wang2023SelfConsistencyImproves}.
Prompting with \ToT addresses this, as we can see multiple thoughts explorer per level independently.
This concrete number is chosen just for illustrative purposes.
If and only if the \LLM identifies them as wrong, they will be discarded.
The output is then reached as a consensus of these multiple paths.

\begin{prompt}{Excerpt of a \ToT prompt by \citewithauthor{hulbert2023UsingTreeofThought}}{tot-1-short}
    \label{prompt:tot_short}
    \\
    Identify and behave as three different experts that are appropriate to answering this question. \\
    \omitt \\
    If any expert is judged to be wrong at any point then they leave. \\
    \omitt
\end{prompt}

The prompts by \citewithauthor{hulbert2023UsingTreeofThought} aim to simplify \ToT prompting into a singular request.
They provide three different prompts to achieve this.
The complete best performing \ToT \autoref{prompt:tot} for \TLR in the \LiSSAf can be found in the appendix.
As illustrated in \autoref{fig:ToT_visualization}, many branches are explored concurrently instead of following a single strain of thoughts like \CoT prompting.
We can see this instruction in \autoref{prompt:tot}.
The \LLM is instructed to act as multiple experts to initiate multiple thought processes concurrently.
Discarding misleading thoughts, symbolised by orange boxes in \autoref{fig:ToT_visualization} is instructed by removing the experts from the simulated discussion.
The benchmark data to compare \ToT prompting with the existing zero-shot and \CoT classifiers in the \LiSSAF for \RtR also exists in public repositories~\cite{fuchss2022ArDoCoBenchmark, hey2025ReplicationPackage}.
Adding this prompting style to the \LiSSAf will broaden the baseline to compare later results and possible improvements against.

The \LiSSAf is publicly accessible in a repository using the MIT license.
Various classifiers are already included there.
To apply the \ToT prompting technique, existing classifiers can be modified in their classification prompt to utilize \autoref{prompt:tot} instead.
Of course the relevant question for \RtR \TLR will be included in the actual prompt as well.
This \ToT classification has two major goals.
Foremost, to get confident and set up with the general project structure and benchmark data.
This is a prerequisite for my future work in the \LiSSA framework.
Second, this broadens the baseline to compare later results and possible improvements against.

\section{\APECapitalized in LiSSA}
\label{sec:Approach:ape_in_lissa}

\begin{figure}
    \centering
    \includesvg[width=\textwidth]{graphics/LiSSA_pipeline_with_optimization.svg}
    \caption{Overview of the proposed prompt optimization approach for \TLR tasks in the \LiSSAf. Modified from \citewithauthor{fuchss2025LiSSAGeneric}}
    \label{fig:prompt_optimization_pipeline}
    The new prompt optimization component is highlighted in light blue.
    Highlighted in yellow are the actual artifacts.
    Highlighted in purple are steps prompting \LLMs for processing.
    White nodes represent processing inside the framework.
\end{figure}

To add \APE capabilities to the \LiSSAF, a new pipeline step is introduced.
\autoref{fig:prompt_optimization_pipeline} provides an overview of the proposed approach.
The new optimization step, highlighted in light blue, is an iterative pipeline step to alter the classification prompt.
It is placed after the initial preprocessing of the input data.
The \promptoptimizer component uses the preprocessed data to optimize a given prompt.
The optimized prompt is then used in the regular \LLM-based prompting step to classify \TLs.

The different prompt optimization algorithms to be used are be introduced in the following subsections.
First a naive iterative approach is presented in \autoref{approach:sec:naive_iterative}.
This implementation is the basis for the more sophisticated \ProTeGi algorithm in \autoref{approach:sec:gradient_descent}.

\subsection{Naive Iterative \APECapitalized}
\label{approach:sec:naive_iterative}
\label{approach:sec:naive_iterative_feedback}
To add \APE capabilities to the \LiSSAF, a naive iterative optimizer is added.
As in \autoref{sec:Approach:tot} the existing classifiers of the \LiSSAF can be used to evaluate the \TLR performance of the optimized prompts.

\begin{prompt}{Simple Optimization Prompt}{initial_optimization}
    \label{prompt:optimization_prompt_simple}
    \\
    \input{prompts/optimization-prompt-simple}
\end{prompt}

To improve \TLR performance, \autoref{prompt:optimization_prompt_simple} is used as the optimization prompt.
As visualized in \autoref{fig:iterative_core_loop} the expected outputs for training data may also be used to improve the prompt.
The most naive approach is to simply ask the \LLM to improve the prompt without providing further direction or information.
Context about why the prompt did not meet the requirements, also referred to as feedback, is not yet provided with \autoref{prompt:optimization_prompt_simple}.
The optimized prompt is taken into the next iteration.
The loop of evaluating and optimizing is repeated, until a threshold value is passed, meaning the prompt is considered good enough.
To limit resource usage, a hard limit is also included to put an upper bound on the number optimization attempts.

This naive \promptoptimizer implementation is used as a basis for more sophisticated prompt optimization algorithms.
Furthermore, we are able to compare whether optimization results differ between naive and sophisticated approaches.

\subsection{\APECapitalized with \ProTeGi}
\label{approach:sec:gradient_descent}
In this subsection feedback is introduced to the optimization process.
The naive iterative optimization implementation created for \autoref{approach:sec:naive_iterative} is used as a foundation for the implementation.
The core concepts of the \ProTeGi optimization algorithm remain the same.
Details of the \ProTeGi algorithm are introduced in \autoref{sec:gradient_descent}.

First the \nameref{approach:sec:naive_iterative} is extended to include simple feedback strings about misclassifications.
The feedback is generated by comparing the actual output of the \LLM classification with the expected output for the training data.
This feedback is then provided to the \LLM in the optimization prompt.
However, this does not include any processing logic to analyze or select the optimized prompts.
This is addressed by implementing the \ProTeGi algorithm.

\begin{align}
    \label{eq:naive_opti}
    f: llm(p, X) \rightarrow [0, 1] \\
    t   \in [0, 1]
\end{align}

We can compare the performance of prompts using a metric function.
More formally as written in \autoref{eq:naive_opti}, a function $f$ will be required to quantify the result, also referred to as performance.
The current prompt $p$ iteration, called to a \LLM $llm$ using a set of training data $X$, returns this result.
The metrics provided by an \LiSSA evaluation run: precision, recall and \fone, can be used as this performance function.
The function $f$ needs to have the same range for any set of training data to ensure the threshold value $t$ will always be viable.

\citewithauthor{pryzant2023AutomaticPrompt} have provided their Python source code in a publicly accessible repository\footnote{https://github.com/microsoft/LMOps/tree/main/prompt\_optimization} under the MIT license.
The project code for their algorithm, is the basis for the adaption to the \LiSSA Java framework.
The prompt candidates are compared with each other using the performance function $f$.
The best performing prompts are then taken into the next iteration of the optimization loop.
Feeding more information and processing logic into the optimization system is expected to present better results than the naive approach.
Especially, suitable results may be reached faster and or cheaper by more processing on the local machine instead of expensive \API calls to the \LLM.