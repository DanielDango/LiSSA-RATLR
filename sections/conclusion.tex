%% LaTeX2e class for student theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability
%% Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.6, 2024-06-07

\chapter{Conclusion}
\label{ch:Conclusion}
\Todo{gerne auch hier noch konkrete zahlen; generell hast du ja auch noch ToT gemacht; das sollte im Abstract und der Conclusion als Beitrag glaub auch noch rein. und generell auch die zwei varianten von APE sollten deutlicher in beidem hervorgehoben werden, da das arbeit von dir war.}

This thesis presents an \APE approach using \LLMs to optimize prompts for \RtR \TLR tasks in the \LiSSAF.
To realize this, a new prompt optimization component is realized for the \LiSSAF.
Among other strategies for \APE, the gradient descent-based algorithm \ProTeGi is implemented.
Utilizing the added \promptoptimizer component, prompts can be refined iteratively to provide better classification results on a training set.
In addition, the naive strategies of prompting the \LLM to optimize the prompt with or without examples of misclassified \TLs are also presented.
The approach is evaluated on five datasets from the \RtR task using four \LLMs.
The results show that the realized approach can improve the performance of \TLR tasks compared to the baseline classification prompts currently used in the \LiSSAF.
At the same time, the absolute difference between the different \APE algorithms \TLR performances is quite small.
Importantly, it is demonstrated that the best-performing optimized prompt in terms of \fone, \autoref{prompt:optimized_classificiation_prompt} for the \WARC dataset also performed well on other datasets of the same task.
This indicates that the optimized prompts may be used as fixed classification prompts in further work or projects, even if the \APE pipeline is not applied.
\autoref{prompt:optimized_classificiation_prompt} was optimized with the \ProTeGi implementation of the \APE component with the zero-shot \KISS \autoref{prompt:yes_no} as initial prompt.

Further work might include applying the \APE component to different domains and tasks.
Establishing generalized optimized prompts with improved performance on multiple domains is desirable, as \LiSSA is not restricted to the \RtR task.
The gradient descent-based \APE algorithm still has many more parameters that have been introduced during development, but have not been systematically altered during evaluation.
Exploring configuration variations, for instance, utilizing greater optimization budgets, is another possibility to build on this work.
Stepping up from \TLR, the optimization prompts used for optimization in the \promptoptimizer component might also be optimized themselves.

As the \LLMs output is not deterministic, the cached requests and responses are included in the \replicationPackage for this thesis.
Alongside the cached requests, the used datasets and code are also publicly available in this replication package as well.
Variances in performance are expected when rerunning the requests instead of using the cached values.