\chapter{Work Plan}

\section{Phases}
\subsection{Initial Overview}

\textbf{What}: To get familiar with the LiSSA framework, I will implement basic classifiers with simple popular prompting techniques.
Currently, a zero-shot and chain-of-thought classifier are implemented.

\textbf{How}: The LiSSA framework is publicly accessible in a repository \footnote{https://github.com/ArDoCo/LiSSA-RATLR/} using the MIT license. This way, existing classifiers can be used as an inspiration for development. 

The existing class \verb|classifier| is an abstraction of all classifiers and provides TODO
T

\textbf{Why}: In order to provide a broader baseline to compare later results against 

\subsection{Naive Iterative Optimization}
Next, I plan to implement a naive iterative approach to prompt optimization. Many automatic prompt optimization algorithms depend on a iterative core loop which will be repeated until the optimized prompt performs better than some metric or a maximum amount of iterations has been reached.

To improve the performance, an optimization prompt is used. The naive approach is to simply prompt the LLM to improve the prompt. The hopefully improved prompt will be taken into the next iteration.

\subsection{Automatic Prompt Optimization Based on Gradient Descent}

\subsection{Evaluation and Buffer}
In order to evaluate the performance of different optimized prompts, the benchmark data from \citeauthor{fuchss2022establishing} \cite{fuchss2022establishing} will be used.

As this work can be seen as an expansion on the recent work of \citewithauthor{fuchss2025lissa} the same metrics will be used. These are the precision, recall, $F_1$-Score and $F_2$-Score. This enables an easy comparison, especially with the manual prompts designed by \citewithauthor{ewald2024retrieval}.

Precision and recall are key measures for information retrieval tasks \cite{hayes2006advancing}. 
\textit{TODO: Explain these metricize and how they are calculated}

Depending on how long each phase will take, it is possible to generate a bunch of different data and perform comparisons.
A simple but interesting approach is to compare different LLMs for the task. Many variations can be achieved by comparing for example how a prompt optimized by one system performs on the others. We can also compare how well each system manages to optimize the initial prompt. 
Another interesting thought is to take optimized prompts from a different system as the initial prompt. 

\section{Artifacts}
\section{Schedule}
